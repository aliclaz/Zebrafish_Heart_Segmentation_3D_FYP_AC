{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from pretrained_seg_models import Unet, AttentionUnet, AttentionResUnet, get_preprocessing, losses, metrics\n",
    "from mpl_toolkits import mplot3d\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from statistical_analysis.maths_stats import get_class_volume_msd, get_CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for dateset and the number of classes in the dataset\n",
    "\n",
    "HPF = 48\n",
    "PATH = '/content/drive/MyDrive/Data/Train{}/'.format(HPF)\n",
    "TEST_PATH = '/content/drive/MyDrive/Data/Test{}/'.format(HPF)\n",
    "OUT_PATH = '/content/drive/MyDrive/Data/Results'\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "DEPTH = 256\n",
    "CHANNELS = 3\n",
    "if HPF == 48:\n",
    "    n_classes = 6\n",
    "elif HPF == 36:\n",
    "    n_classes = 5\n",
    "elif HPF == 30:\n",
    "    n_classes = 4\n",
    "else:\n",
    "    n_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "\n",
    "encoder_weights = 'imagenet'\n",
    "BACKBONE1 = 'resnet34'\n",
    "activation = 'softmax'\n",
    "patch_size = 64\n",
    "channels = 3\n",
    "\n",
    "LR = 0.0001\n",
    "opt = Adam(LR)\n",
    "\n",
    "flat_train_masks = train_masks.reshape(-1)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(flat_train_masks), y=flat_train_masks)\n",
    "\n",
    "DL = losses.DiceLoss(class_weights=class_weights)\n",
    "FL = losses.CategoricalFocalLoss()\n",
    "total_loss = DL + (1 * FL)\n",
    "\n",
    "metrics = [metrics.IOUScore(threshold=0.5), metrics.FScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data\n",
    "\n",
    "preprocess_input1 = get_preprocessing(BACKBONE1)\n",
    "x_train_prep = preprocess_input1(x_train)\n",
    "x_val_prep = preprocess_input1(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - using AttentionResUnet with a resnet34 backbone and \n",
    "# pretrained weights\n",
    "\n",
    "model1 = AttentionResUnet(BACKBONE1, classes=n_classes, \n",
    "                            input_shape=(patch_size, patch_size, patch_size, channels), \n",
    "                            encoder_weights=encoder_weights, activation=activation)\n",
    "model1.compile(optimizer=opt, loss=total_loss, metrics=metrics)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history1 = model1.fit(x_train_prep, y_train, batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=(x_val_prep, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for use in the future\n",
    "\n",
    "model1.save(OUT_PATH+'{}HPF_{}_atten_resunet_100epochs.h5'.format(HPF, BACKBONE1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss and accuracy at each epoch\n",
    "\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for AttentionResUNet with ResNet34 backbone')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history1.history['iou_score']\n",
    "val_acc = history1.history['val_iou_score']\n",
    "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation IOU ')\n",
    "plt.title('Training and Validation IOU for AttentionResUNet with ResNet34 backbone')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IOU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data with vgg16 backbone\n",
    "\n",
    "BACKBONE2 = 'vgg16'\n",
    "\n",
    "preprocess_input2 = get_preprocessing(BACKBONE2)\n",
    "x_train_prep = preprocess_input2(x_train)\n",
    "x_val_prep = preprocess_input2(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - using AttentionUnet with a vgg16 backbone and \n",
    "# pretrained weights\n",
    "\n",
    "model2 = AttentionUnet(BACKBONE2, classes=n_classes, \n",
    "                            input_shape=(patch_size, patch_size, patch_size, channels), \n",
    "                            encoder_weights=encoder_weights, activation=activation)\n",
    "model2.compile(optimizer=opt, loss=total_loss, metrics=metrics)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history2 = model2.fit(x_train_prep, y_train, batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=(x_val_prep, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss and accuracy at each epoch\n",
    "\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for AttentionUNet with VGG16 backbone')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history2.history['iou_score']\n",
    "val_acc = history2.history['val_iou_score']\n",
    "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
    "plt.title('Training and Validation IOU for AttentionUNet with VGG16 backbone')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IOU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - using Unet with a vgg16 backbone and \n",
    "# pretrained weights\n",
    "\n",
    "model3 = Unet(BACKBONE2, classes=n_classes, \n",
    "                            input_shape=(patch_size, patch_size, patch_size, channels), \n",
    "                            encoder_weights=encoder_weights, activation=activation)\n",
    "model3.compile(optimizer=opt, loss=total_loss, metrics=metrics)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history3 = model3.fit(x_train_prep, y_train, batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=(x_val_prep, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss and accuracy at each epoch\n",
    "\n",
    "loss = history3.history['loss']\n",
    "val_loss = history3.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for UNet with VGG16 backbone')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history3.history['iou_score']\n",
    "val_acc = history3.history['val_iou_score']\n",
    "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
    "plt.title('Training and Validation IOU for UNet with VGG16 backbone')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IOU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual and predicted masks for each patch in the validation set\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4*len(y_val)))\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    ax = fig.add_subplot(len(y_val), 2, (2*i)+1, projection='3d')\n",
    "    ax.set_title('Actual Mask')\n",
    "    for j in range(n_classes):\n",
    "        if j != 0:\n",
    "            points = np.nonzero(y_val[i,:,:,:,0] == 0.2*j)\n",
    "            y = points[0]\n",
    "            x = points[1]\n",
    "            z = points[2]\n",
    "            c1 = np.full(len(y), '{}'.format(0.2*j), dtype=np.float32)\n",
    "            if y.shape[0] != 0:\n",
    "                ax.scatter(x, y, z, c=c1, cmap='gray', alpha=1)\n",
    "\n",
    "    ax = fig.add_subplot(len(y_val), 2, (2*i)+2, projection='3d')\n",
    "    ax.set_title('Predicted Mask')\n",
    "    for j in range(n_classes):\n",
    "        if j != 0:\n",
    "            points = np.nonzero(val_preds[i,:,:,:,0] == 0.2*j)\n",
    "            y = points[0]\n",
    "            x = points[1]\n",
    "            z = points[2]\n",
    "            c2 = np.full(len(y), '{}'.format(0.2*j), dtype=np.float32)\n",
    "            if y.shape[0] != 0:\n",
    "                ax.scatter(x, y, z, c=c2, cmap='gray', alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for testing and predictions\n",
    "\n",
    "my_model = load_model(OUT_PATH+'{}HPF_resnet34_atten_resunet_100epochs.h5'.format(HPF), compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imsave\n",
    "\n",
    "# Save masks as segmented volumes\n",
    "\n",
    "for i in reconstructed_imgs:\n",
    "    imsave(OUT_PATH+'fish{}_{}HPF.tif'.format(i, HPF), reconstructed_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation of the volume of each area of the heart\n",
    "# for a healthy fish (no GM) at each stage of development\n",
    "\n",
    "if HPF == 48:\n",
    "    classes = ['Background', 'Noise', 'Endocardium', 'Atrium', 'AVC', 'Ventricle']\n",
    "    train_masks = np.expand_dims(, axis=4)\n",
    "    healthy_masks = np.concatenate((train_masks, reconstructed_imgs), axis=0)\n",
    "    healthy_scales = [295.53, 233.31, 233.31, 246.27, 246.27]\n",
    "elif HPF == 36:\n",
    "    classes = ['Background', 'Noise', 'Endocardium', 'Atrium', 'Ventricle']\n",
    "    train_masks = np.expand_dims(masks, axis=4)\n",
    "    healthy_masks = np.concatenate((train_masks, reconstructed_imgs), axis=0)\n",
    "    healthy_scales = [221.65, 221.65, 221.65, 221.65, 221.65, 221.65]\n",
    "elif HPF == 30:\n",
    "    classes = ['Background', 'Noise', 'Endocardium', 'Linear Heart Tube']\n",
    "    train_masks = np.expand_dims(masks, axis=4)\n",
    "    healthy_masks = np.concatenate((train_masks, reconstructed_imgs), axis=0)\n",
    "    healthy_scales = [221.65, 221.65]\n",
    "\n",
    "msd_class_vols_df = get_class_volume_msd(healthy_masks, classes, healthy_scales, '{}'.format(HPF), 'Healthy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the means and standard deviations of each class to calculate the\n",
    "# confidence intervals of the healthy volumes\n",
    "\n",
    "sds = msd_class_vols_df['Standard Deviation of Healthy Volume (\\u03bcm\\u00b2) at {}HPF'.format(HPF)].tolist()\n",
    "class_vol_CIs_df = get_CIs(sds, len(healthy_masks), classes, '{}'.format(HPF), 'Healthy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove background, noise and endocardium rows as these aren't of interest and were only included\n",
    "# so the model produces better predictions of the other classes\n",
    "\n",
    "msd_class_vols_df = msd_class_vols_df.drop(['Background', 'Noise', 'Endocardium'])\n",
    "class_vol_CIs_df = class_vol_CIs_df.drop(['Background', 'Noise', 'Endocardium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all information into one dataframe, display it and save it as a CSV file\n",
    "# which can be viewed outside of the code or terminal\n",
    "\n",
    "stats = pd.DataFrame()\n",
    "stats = stats.append(msd_class_vols_df)\n",
    "df = stats.append(class_vol_CIs_df)\n",
    "\n",
    "print(stats)\n",
    "\n",
    "stats.to_csv(OUT_PATH+'{}HPF_stat_results.csv'.format(HPF))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
