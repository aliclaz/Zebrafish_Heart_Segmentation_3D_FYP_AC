{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.utils import compute_class_weight\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgPreprocessing import load_process_imgs\n",
    "from pretrained_seg_models import Unet, AttentionUnet, AttentionResUnet, get_preprocessing, losses, metrics\n",
    "from display import show_history, show_all_historys, show_val_masks, show_test_masks\n",
    "from predict_module import val_predict, predict\n",
    "from statistical_analysis.df_manipulation import healthy_df_calcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for dateset and the number of classes in the dataset\n",
    "\n",
    "hpf = 48\n",
    "img_path = '/content/drive/MyDrive/Data/Train{}/Images/'.format(hpf)\n",
    "mask_path = '/content/drive/MyDrive/Data/Train{}/Masks/'.format(hpf)\n",
    "test_path = '/content/drive/MyDrive/Data/Test{}/'.format(hpf)\n",
    "out_path = '/content/drive/MyDrive/Data/Results'\n",
    "if hpf == 48:\n",
    "    n_classes = 6\n",
    "elif hpf == 36:\n",
    "    n_classes = 5\n",
    "elif hpf == 30:\n",
    "    n_classes = 4\n",
    "else:\n",
    "    n_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training masks and images into the code and preprocess both datasets\n",
    "\n",
    "x_train, x_val, y_train, y_val = load_process_imgs(img_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "\n",
    "encoder_weights = 'imagenet'\n",
    "backbone1 = 'resnet34'\n",
    "activation = 'softmax'\n",
    "patch_size = 64\n",
    "channels = 3\n",
    "\n",
    "LR = 0.0001\n",
    "opt = Adam(LR)\n",
    "\n",
    "train_masks = np.concatenate(y_train, y_val)\n",
    "flat_train_masks = train_masks.reshape(-1)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(flat_train_masks), y=flat_train_masks)\n",
    "\n",
    "total_loss = losses.DiceLoss(class_weights=class_weights) + losses.CategoricalFocalLoss()\n",
    "\n",
    "metrics = [metrics.IOUScore(threshold=0.5), metrics.FScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data\n",
    "\n",
    "preprocess_input1 = get_preprocessing(backbone1)\n",
    "x_train_prep = preprocess_input1(x_train)\n",
    "x_val_prep = preprocess_input1(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - using AttentionResUnet with a resnet34 backbone and \n",
    "# pretrained weights\n",
    "\n",
    "model1 = AttentionResUnet(backbone1, classes=n_classes, \n",
    "                            input_shape=(patch_size, patch_size, patch_size, channels), \n",
    "                            encoder_weights=encoder_weights, activation=activation)\n",
    "model1.compile(optimizer=opt, loss=total_loss, metrics=metrics)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history1 = model1.fit(x_train_prep, y_train, batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=(x_val_prep, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of model names, historys and backbones used\n",
    "\n",
    "models = model_names = historys = backbones = []\n",
    "\n",
    "models.append(model1)\n",
    "model_name1 = 'AttentionResUnet'\n",
    "model_names.append(model_name1)\n",
    "historys.append(history1)\n",
    "backbones.append(backbone1)\n",
    "\n",
    "# Plot the train and validation losses and IOU scores at each epoch for model 1\n",
    "\n",
    "show_history(history1, model_name1, backbone1, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for use in the future\n",
    "\n",
    "model1.save(out_path+'{}HPF_{}_{}_100epochs.h5'.format(hpf, backbone1, model_name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data with vgg16 backbone\n",
    "\n",
    "backbone2 = 'vgg16'\n",
    "\n",
    "preprocess_input2 = get_preprocessing(backbone2)\n",
    "x_train_prep = preprocess_input2(x_train)\n",
    "x_val_prep = preprocess_input2(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - using AttentionUnet with a vgg16 backbone and \n",
    "# pretrained weights\n",
    "\n",
    "model2 = AttentionUnet(backbone2, classes=n_classes, \n",
    "                            input_shape=(patch_size, patch_size, patch_size, channels), \n",
    "                            encoder_weights=encoder_weights, activation=activation)\n",
    "model2.compile(optimizer=opt, loss=total_loss, metrics=metrics)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history2 = model2.fit(x_train_prep, y_train, batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=(x_val_prep, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of model names, historys and backbones used\n",
    "\n",
    "models.append(model2)\n",
    "model_name2 = 'AttentionUnet'\n",
    "model_names.append(model_name2)\n",
    "historys.append(history2)\n",
    "backbones.append(backbone2)\n",
    "\n",
    "# Plot train and validation losses and IOU scores for model 2\n",
    "\n",
    "show_history(history2, model2, backbone2, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(out_path+'{}HPF_{}_{}_100epochs.h5'.format(hpf, backbone2, model_name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - using Unet with a vgg16 backbone and \n",
    "# pretrained weights\n",
    "\n",
    "model3 = Unet(backbone2, classes=n_classes, \n",
    "                            input_shape=(patch_size, patch_size, patch_size, channels), \n",
    "                            encoder_weights=encoder_weights, activation=activation)\n",
    "model3.compile(optimizer=opt, loss=total_loss, metrics=metrics)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history3 = model3.fit(x_train_prep, y_train, batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=(x_val_prep, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of model names, historys and backbones used\n",
    "\n",
    "models.append(model3)\n",
    "model_name3 = 'Unet'\n",
    "model_names.append(model_name3)\n",
    "historys.append(history3)\n",
    "backbones.append(backbone2)\n",
    "\n",
    "# Plot the train and validation losses and IOU scores at each epoch for model 3\n",
    "\n",
    "show_history(history3, model_name3, backbone2, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model3.save(out_path+'{}HPF_{}_{}_100epochs.h5'.format(hpf, backbone2, model_name3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the historys of all models together for comparison\n",
    "\n",
    "show_all_historys(historys, model_names, backbones, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use each model to predict masks for each validation image\n",
    "\n",
    "val_preds_each_model = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    val_preds_each_model.append(val_predict(models[i], x_val, 64))\n",
    "val_preds_each_model = np.array(val_preds_each_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the validation images, their actual masks and their masks predicted by each model at 3 slices\n",
    "\n",
    "show_val_masks(model_names, x_val, y_val, val_preds_each_model, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use each model to predict masks for each test image\n",
    "\n",
    "test_preds_each_model = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    test_imgs, test_preds = predict(models[i], backbones[i], test_path, out_path)\n",
    "    test_preds_each_model.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display test images, actual masks and predicted masks from each model\n",
    "\n",
    "show_test_masks(model_names, test_imgs, test_preds_each_model, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class labels for each stage of development\n",
    "\n",
    "if hpf == 48:\n",
    "    classes = ['Background', 'Noise', 'Endocardium', 'Atrium', 'AVC', 'Ventricle']\n",
    "    train_masks = np.expand_dims(train_masks, axis=4)\n",
    "    healthy_masks = np.concatenate((train_masks, test_imgs), axis=0)\n",
    "    healthy_scales = [295.53, 233.31, 233.31, 246.27, 246.27]\n",
    "elif hpf == 36:\n",
    "    classes = ['Background', 'Noise', 'Endocardium', 'Atrium', 'Ventricle']\n",
    "    train_masks = np.expand_dims(train_masks, axis=4)\n",
    "    healthy_masks = np.concatenate((train_masks, test_imgs), axis=0)\n",
    "    healthy_scales = [221.65, 221.65, 221.65, 221.65, 221.65, 221.65]\n",
    "elif hpf == 30:\n",
    "    classes = ['Background', 'Noise', 'Endocardium', 'Linear Heart Tube']\n",
    "    train_masks = np.expand_dims(train_masks, axis=4)\n",
    "    healthy_masks = np.concatenate((train_masks, test_imgs), axis=0)\n",
    "    healthy_scales = [221.65, 221.65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the means, standard deviations and confidence intervals of the volume of each class\n",
    "# Put these into a dataframe, display it and then save it as a CSV for access from the main program\n",
    "\n",
    "healthy_df_calcs(healthy_masks, classes, scales, hpf, out_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
